[
    {
        "qid": "greedy_algorithm-e-001",
        "short_answer": "Greedy algorithms make locally optimal choices at each step.",
        "long_answer": "A greedy algorithm is an algorithmic strategy that builds a solution incrementally by always making the locally optimal choice at each step, with the hope that these local decisions will lead to a globally optimal solution. The defining characteristic of greedy algorithms is that once a choice is made, it is never reconsidered or revised.\n\nGreedy algorithms are attractive because of their simplicity and efficiency. They often lead to clean implementations with low time and space complexity, making them suitable for large-scale problems. However, this simplicity comes at a cost: greedy algorithms do not explore all possible solutions and therefore may fail on problems that require global reasoning.\n\nThe success of a greedy algorithm depends heavily on the structure of the problem. If a problem satisfies the greedy choice property, then a locally optimal choice can be proven to be part of a globally optimal solution. Without this property, greedy algorithms may get stuck in suboptimal solutions.\n\nIn extended explanations, candidates often discuss examples where greedy works well, such as interval scheduling and Huffman coding, as well as counterexamples where greedy fails. These discussions usually appear later in long answers.\n\nIf an evaluation system truncates the response early, it may miss this critical distinction between local and global optimality. Fusion-based semantic evaluation ensures the full conceptual reasoning is captured."
      },
      {
        "qid": "greedy_algorithm-e-002",
        "short_answer": "Greedy algorithms require greedy choice and optimal substructure.",
        "long_answer": "For a greedy algorithm to produce an optimal solution, the problem must satisfy two key properties: the greedy choice property and optimal substructure. The greedy choice property ensures that making the locally optimal choice at each step leads toward a globally optimal solution.\n\nOptimal substructure means that an optimal solution to the problem contains optimal solutions to its subproblems. Once a greedy choice is made, the remaining problem must still be optimally solvable using the same strategy.\n\nThese two properties distinguish problems where greedy works from those that require more exhaustive approaches such as dynamic programming. Many problems exhibit optimal substructure but fail the greedy choice property, making greedy algorithms unsuitable.\n\nExtended explanations often include formal proofs using exchange arguments or cut-and-paste techniques to justify greedy correctness. These proofs are typically presented later in detailed responses.\n\nIf truncated early, an evaluator may incorrectly assume greedy always works when optimal substructure exists. Fusion-based semantic evaluation ensures these subtle but crucial distinctions are preserved."
      },
      {
        "qid": "greedy_algorithm-e-003",
        "short_answer": "Greedy coin change picks the largest possible denomination.",
        "long_answer": "The coin change problem asks for the minimum number of coins needed to make a given amount using specified denominations. A greedy approach repeatedly selects the largest denomination coin that does not exceed the remaining amount.\n\nThis strategy works optimally for standard coin systems such as U.S. currency (1, 5, 10, 25) because the denominations are designed to satisfy the greedy choice property. However, for arbitrary denominations, greedy may fail. For example, with denominations {1, 3, 4}, greedy fails for amount 6 by choosing 4 + 1 + 1 instead of optimal 3 + 3.\n\nThis example illustrates the importance of validating greedy assumptions before applying the algorithm. Simply because greedy works in one context does not mean it generalizes.\n\nExtended explanations often analyze why certain coin systems are canonical and why others are not. These nuanced discussions often appear later in long responses.\n\nFusion-based semantic evaluation ensures that these critical caveats are retained even when answers exceed typical token limits."
      },
      {
        "qid": "greedy_algorithm-e-004",
        "short_answer": "Activity selection uses earliest finishing time.",
        "long_answer": "The Activity Selection Problem involves selecting the maximum number of non-overlapping activities given their start and finish times. The greedy solution sorts activities by their finish times and always selects the activity that finishes earliest.\n\nThis greedy choice is optimal because selecting the activity that finishes first leaves the maximum remaining time for subsequent activities. Once this choice is made, the remaining problem is identical in structure, satisfying optimal substructure.\n\nThe algorithm runs in O(n log n) time due to sorting and O(n) for selection. It is one of the most commonly cited examples used to demonstrate greedy correctness proofs.\n\nExtended explanations often include exchange arguments showing that any optimal solution can be transformed to match the greedy solution without reducing optimality.\n\nFusion-based semantic evaluation ensures that the reasoning behind why earliest finishing time works is fully preserved."
      },
      {
        "qid": "greedy_algorithm-e-005",
        "short_answer": "Fractional knapsack greedily selects highest value-to-weight items.",
        "long_answer": "The Fractional Knapsack problem is a classic optimization problem where items can be divided into fractions. The goal is to maximize total value while staying within a fixed weight capacity. Unlike the 0/1 Knapsack problem, fractional selection allows partial inclusion of items.\n\nThe greedy strategy sorts all items by their value-to-weight ratio in descending order. Items with the highest ratio are chosen first because they provide the greatest value per unit weight. If an item cannot be fully included due to weight constraints, only the fraction that fits is taken.\n\nThis greedy approach is provably optimal for the fractional version of the knapsack problem because the objective function is linear and divisible. Each greedy choice locally maximizes value gain without restricting future choices.\n\nExtended explanations often contrast this with the 0/1 knapsack problem, where greedy fails and dynamic programming is required. Fusion-based semantic evaluation ensures these distinctions are preserved even in long answers."
      },
      {
        "qid": "greedy_algorithm-e-006",
        "short_answer": "Huffman coding greedily merges lowest-frequency symbols.",
        "long_answer": "Huffman coding is a greedy algorithm used to generate optimal prefix-free binary codes for symbols based on their frequencies. The algorithm repeatedly selects the two symbols with the lowest frequencies and merges them into a single node.\n\nThis greedy choice ensures that symbols with lower frequencies end up deeper in the tree, resulting in longer codes, while high-frequency symbols receive shorter codes. The final tree minimizes the weighted average code length, which is optimal.\n\nHuffman coding is widely used in data compression formats such as ZIP, JPEG, and MP3. Its correctness is supported by a greedy-choice proof that shows merging the two least frequent symbols is always safe.\n\nExtended explanations often include proofs of optimality and discussions of prefix codes. Fusion-based semantic evaluation ensures that these deeper theoretical insights are not truncated."
      },
      {
        "qid": "greedy_algorithm-e-007",
        "short_answer": "Job scheduling greedily selects jobs by profit.",
        "long_answer": "The Job Scheduling with Deadlines problem aims to maximize total profit by scheduling jobs, each with a deadline and profit, such that at most one job is performed at a time. The greedy strategy sorts jobs in descending order of profit.\n\nEach job is then assigned to the latest available time slot before its deadline. This ensures that high-profit jobs are prioritized while leaving earlier slots available for other jobs.\n\nThe algorithm typically runs in O(n log n) time due to sorting, with additional overhead for managing time slots. Its correctness relies on the greedy choice of selecting the most profitable job first.\n\nExtended explanations often analyze why earliest slots should be preserved and how this greedy approach avoids conflicts. Fusion-based semantic evaluation ensures these scheduling insights are fully captured."
      },
      {
        "qid": "greedy_algorithm-e-008",
        "short_answer": "Greedy and DP differ in flexibility and optimality guarantees.",
        "long_answer": "Greedy algorithms and dynamic programming differ fundamentally in how they approach problem solving. Greedy algorithms make immediate local choices without reconsideration, aiming for efficiency and simplicity.\n\nDynamic programming, in contrast, explores all relevant subproblems and guarantees global optimality by combining optimal sub-solutions. This makes DP more powerful but often more computationally expensive.\n\nGreedy algorithms typically run in O(n log n) time and use minimal space, while DP may require polynomial time and space. Greedy is preferred when the greedy choice property holds; otherwise, DP is necessary.\n\nExtended discussions often compare specific problems where greedy fails but DP succeeds. Fusion-based semantic evaluation ensures that these nuanced trade-offs are not lost."
      },
      {
        "qid": "greedy_algorithm-e-009",
        "short_answer": "Greedy correctness is proven using exchange arguments.",
        "long_answer": "Proving the correctness of a greedy algorithm is often more challenging than implementing it. The most common technique used is the exchange argument, which shows that any optimal solution can be transformed into a solution that includes the greedy choice without reducing optimality.\n\nThe idea is to assume an optimal solution that differs from the greedy solution at some point. Then, by exchanging elements in the optimal solution with the greedy choice, we show that the modified solution is still optimal. This proves that the greedy choice is safe.\n\nAnother common proof technique is the cut-and-paste method, where the problem is divided into independent parts, and greedy choices are shown to be optimal for each part. These proofs rely heavily on problem structure.\n\nExtended explanations often include formal proofs for classical problems like activity selection and Huffman coding. Fusion-based semantic evaluation ensures these later-stage theoretical arguments are not lost due to truncation."
      },
      {
        "qid": "greedy_algorithm-e-010",
        "short_answer": "MST connects all vertices with minimum total edge weight.",
        "long_answer": "The Minimum Spanning Tree (MST) problem asks for a subset of edges that connects all vertices of a weighted graph while minimizing the total edge weight. The resulting structure must be acyclic and include all vertices.\n\nTwo classic greedy algorithms solve the MST problem: Kruskal’s algorithm and Prim’s algorithm. Kruskal’s algorithm sorts edges by weight and adds them if they do not form a cycle. Prim’s algorithm grows the tree incrementally from a starting vertex by selecting the minimum-weight edge connecting the tree to a new vertex.\n\nBoth algorithms rely on the cut property, which guarantees that the minimum-weight edge crossing any cut is safe to include. This property underpins their correctness.\n\nExtended explanations often compare implementation details and performance trade-offs. Fusion-based semantic evaluation ensures these foundational ideas are preserved."
      },
      {
        "qid": "greedy_algorithm-e-011",
        "short_answer": "Gas station greedy tracks cumulative fuel balance.",
        "long_answer": "The Gas Station problem asks whether it is possible to complete a circular route given two arrays: gas available at each station and the cost required to travel to the next station. The greedy approach relies on tracking cumulative fuel balance.\n\nIf the fuel balance becomes negative at any point, the current starting station cannot be valid. The algorithm then resets the starting point to the next station and continues. If the total gas across all stations is at least the total cost, a solution exists.\n\nThis greedy strategy works because any deficit encountered before a valid starting point invalidates all intermediate starting points. The algorithm runs in O(n) time and uses constant space.\n\nExtended explanations often discuss why the total gas condition is necessary and sufficient. Fusion-based semantic evaluation ensures these insights remain intact."
      },
      {
        "qid": "greedy_algorithm-e-012",
        "short_answer": "Jump Game greedily tracks farthest reachable index.",
        "long_answer": "The Jump Game problem determines whether it is possible to reach the last index of an array where each element represents the maximum jump length from that position. The greedy approach maintains the farthest index that can be reached so far.\n\nAs the algorithm iterates through the array, it updates the farthest reachable position. If at any point the current index exceeds this reach, the algorithm concludes that the end cannot be reached.\n\nThis greedy strategy works because extending reach as far as possible at each step maximizes future options. The algorithm runs in linear time and constant space.\n\nExtended explanations often analyze why dynamic programming is unnecessary here and how greedy dominates. Fusion-based semantic evaluation preserves these conclusions."
      },
      {
        "qid": "greedy_algorithm-e-013",
        "short_answer": "Dijkstra’s algorithm greedily selects the closest unvisited vertex.",
        "long_answer": "Dijkstra’s algorithm is a classic example of a greedy strategy applied to graph problems. The algorithm computes the shortest path from a single source to all other vertices in a graph with non-negative edge weights. The greedy choice is selecting, at each step, the unvisited vertex with the smallest known distance from the source.\n\nOnce a vertex is selected, its distance is finalized, meaning no shorter path to that vertex will be found later. This is guaranteed by the non-negative edge weight condition, which ensures that extending paths cannot reduce the total distance.\n\nAfter selecting the vertex, the algorithm relaxes all outgoing edges, updating distances of neighboring vertices if a shorter path is found. This process continues until all vertices are processed or the destination is reached.\n\nAlthough Dijkstra’s algorithm can be seen as dynamic programming with relaxation, its correctness relies on the greedy choice property. If the greedy selection were incorrect, the algorithm would fail, but mathematical proofs show that this choice is always safe.\n\nExtended answers often discuss priority queue optimizations, comparisons with Bellman-Ford, and real-world routing applications. Fusion-based semantic evaluation ensures that such later-stage reasoning is not ignored when responses exceed token limits."
      },
      {
        "qid": "greedy_algorithm-e-014",
        "short_answer": "Greedy algorithms are fast but not always optimal.",
        "long_answer": "Greedy algorithms offer several advantages, the most significant being simplicity and efficiency. They typically run in linear or logarithmic time and require minimal auxiliary space. This makes them attractive for large-scale problems where performance is critical.\n\nHowever, greedy algorithms also have notable disadvantages. They do not always produce optimal solutions, especially for problems where local optimal choices do not lead to a global optimum. This limitation makes greedy approaches unreliable unless the problem structure explicitly supports them.\n\nAnother challenge is proving correctness. Unlike dynamic programming, where exhaustive exploration ensures optimality, greedy algorithms require rigorous mathematical proofs such as exchange arguments or cut properties.\n\nIn practice, greedy algorithms are often used as heuristics or approximation algorithms for NP-hard problems. Their speed makes them valuable even when exact optimality cannot be guaranteed.\n\nExtended explanations often compare greedy algorithms with DP and backtracking, highlighting trade-offs. Fusion-based semantic evaluation ensures these nuanced comparisons are retained during assessment."
      },
      {
        "qid": "greedy_algorithm-e-015",
        "short_answer": "Greedy load balancing assigns tasks to least-loaded machines.",
        "long_answer": "The Load Balancing problem involves distributing tasks across multiple machines in a way that minimizes the maximum load on any machine. A common greedy approach sorts tasks in descending order of size and assigns each task to the machine with the current minimum load.\n\nThis strategy intuitively balances workloads by preventing large tasks from accumulating on the same machine. While the solution is not always optimal, it provides a provable approximation guarantee, often within a factor of two of the optimal solution.\n\nThe greedy algorithm runs efficiently and is easy to implement, making it suitable for real-world systems such as cloud computing, scheduling, and distributed processing.\n\nHowever, optimal load balancing is NP-hard in general, meaning greedy solutions trade optimality for scalability and speed.\n\nExtended responses often discuss approximation ratios and comparisons with exact algorithms. Fusion-based semantic evaluation ensures that such analytical depth is not lost due to truncation."
      },
      {
        "qid": "greedy_algorithm-e-016",
        "short_answer": "Interval scheduling greedily selects earliest finishing intervals.",
        "long_answer": "The Interval Scheduling Maximization problem aims to select the maximum number of non-overlapping intervals from a given set. The greedy solution sorts intervals by their end times and repeatedly selects the interval that finishes earliest.\n\nThis greedy choice is optimal because selecting the interval with the earliest finish leaves the most room for remaining intervals. Any optimal solution can be transformed to include this greedy choice without reducing the total number of intervals.\n\nThe algorithm runs in O(n log n) time due to sorting and uses minimal extra space. It is one of the most well-known examples where greedy algorithms provably yield optimal solutions.\n\nThis problem is often used as a teaching example for greedy correctness proofs using exchange arguments.\n\nExtended explanations may compare this problem with weighted interval scheduling, where greedy fails and dynamic programming is required. Fusion-based semantic evaluation ensures such distinctions are preserved."
      },
      {
        "qid": "greedy_algorithm-e-017",
        "short_answer": "Minimum platforms are computed using sorted arrivals and departures.",
        "long_answer": "The Minimum Number of Platforms problem determines the minimum number of platforms required at a railway station so that no train waits. Each train has an arrival and a departure time, and platforms cannot be shared simultaneously.\n\nA greedy approach solves this efficiently by sorting arrival times and departure times separately. Two pointers are used to traverse both arrays. When the next train arrives before or at the same time as the earliest departing train, an additional platform is required. When a train departs before the next arrival, a platform is freed.\n\nThe maximum number of platforms required at any time during this process is the answer. This greedy strategy works because it always considers the earliest possible conflict between arrivals and departures.\n\nThe algorithm runs in O(n log n) time due to sorting and uses O(n) space. It is widely applied in scheduling and resource allocation problems.\n\nExtended explanations often discuss edge cases and comparisons with interval partitioning. Fusion-based semantic evaluation ensures these details are captured even in long responses."
      },
      {
        "qid": "greedy_algorithm-e-018",
        "short_answer": "Page replacement uses greedy rules like FIFO, LRU, and Optimal.",
        "long_answer": "The Page Replacement problem arises in operating systems when memory is full and a page must be replaced. Greedy strategies decide which page to evict based on specific rules.\n\nFIFO replaces the oldest loaded page, LRU replaces the least recently used page, and the Optimal strategy replaces the page that will not be used for the longest time in the future. Among these, the Optimal algorithm is theoretically greedy and guarantees minimum page faults, but it is impractical because it requires future knowledge.\n\nLRU is a practical greedy approximation that performs well in real systems. FIFO is simpler but can suffer from anomalies.\n\nThis problem highlights how greedy algorithms can be optimal, approximate, or heuristic depending on available information.\n\nExtended discussions often compare these policies empirically. Fusion-based semantic evaluation ensures deeper reasoning is preserved."
      },
      {
        "qid": "greedy_algorithm-e-019",
        "short_answer": "Meeting rooms are allocated greedily using earliest finishing times.",
        "long_answer": "The Meeting Rooms problem asks for the minimum number of rooms required to schedule all meetings without overlap. A greedy solution sorts meetings by start time and uses a min-heap to track end times of ongoing meetings.\n\nFor each meeting, if its start time is greater than or equal to the earliest ending meeting, the same room can be reused. Otherwise, a new room is allocated. This greedy choice minimizes room usage at every step.\n\nThe algorithm runs in O(n log n) time due to heap operations and sorting. It efficiently handles large schedules and is widely used in calendar systems and resource allocation software.\n\nThe greedy strategy works because it always reuses rooms whenever possible instead of prematurely allocating new ones.\n\nExtended explanations often relate this problem to interval partitioning. Fusion-based semantic evaluation ensures complete understanding of scheduling logic."
      },
      {
        "qid": "greedy_algorithm-e-020",
        "short_answer": "Candy distribution uses two-pass greedy scanning.",
        "long_answer": "The Candy Distribution problem requires assigning candies to children based on ratings such that each child has at least one candy and higher-rated children have more candies than their neighbors.\n\nA greedy two-pass solution is used. In the first pass from left to right, children with higher ratings than their left neighbor receive more candies. In the second pass from right to left, the same rule is applied for right neighbors.\n\nThe final number of candies for each child is the maximum from both passes. This ensures both constraints are satisfied while minimizing total candies.\n\nThe algorithm runs in O(n) time and O(n) space, making it highly efficient.\n\nExtended answers often include correctness proofs and comparisons with brute-force approaches. Fusion-based semantic evaluation ensures these logical arguments are not truncated."
      },
      {
        "qid": "greedy_algorithm-m-001",
        "short_answer": "Task scheduling uses greedy selection with cooldown handling.",
        "long_answer": "The Task Scheduler problem involves arranging tasks with cooldown constraints such that the same type of task cannot be executed again until a fixed cooldown period has passed. The objective is to minimize total execution time, including idle slots if necessary.\n\nA greedy solution uses a max-heap to always select the task with the highest remaining frequency, since executing frequent tasks earlier reduces the likelihood of future idle times. After executing a task, it is placed into a cooldown queue and can only be reinserted into the heap once its cooldown expires.\n\nThis approach ensures that at each step, the most constrained task is handled first, which aligns with the greedy choice property. If no task is available due to cooldowns, the algorithm inserts an idle interval.\n\nThe time complexity depends on the number of tasks and cooldown period, but the algorithm is efficient in practice and widely used in CPU scheduling simulations.\n\nExtended answers often discuss heap-based optimizations and idle-time minimization strategies. Fusion-based semantic evaluation ensures such scheduling rationale is not lost when answers exceed token limits."
      },
      {
        "qid": "greedy_algorithm-m-002",
        "short_answer": "Kruskal’s algorithm builds MST by greedily adding edges.",
        "long_answer": "Kruskal’s algorithm constructs a Minimum Spanning Tree by repeatedly adding the smallest weight edge that does not form a cycle. The greedy choice is always selecting the minimum available edge globally.\n\nTo efficiently detect cycles, the algorithm uses the Union-Find (Disjoint Set) data structure with path compression and union by rank optimizations. These optimizations ensure near-constant amortized time for union and find operations.\n\nEdges are first sorted by weight, giving an overall time complexity of O(E log E). The algorithm continues until exactly V−1 edges are selected.\n\nKruskal’s algorithm is particularly effective for sparse graphs and demonstrates how greedy choices combined with efficient data structures lead to optimal solutions.\n\nExtended discussions include comparisons with Prim’s algorithm and proofs based on cut properties. Fusion-based semantic evaluation preserves these deeper correctness arguments."
      },
      {
        "qid": "greedy_algorithm-m-003",
        "short_answer": "Jump Game II uses greedy range expansion.",
        "long_answer": "The Jump Game II problem asks for the minimum number of jumps required to reach the last index of an array, where each element represents the maximum jump length from that position.\n\nA greedy approach maintains two ranges: the current reachable range and the farthest reachable position within that range. As the algorithm scans through the array, it updates the farthest reachable index. When the current index exceeds the current range, a jump is made, and the range is updated.\n\nThis greedy strategy ensures that each jump maximizes future reach, minimizing the total number of jumps. The algorithm runs in O(n) time and O(1) extra space.\n\nThe correctness relies on the fact that delaying jumps until absolutely necessary never worsens the outcome.\n\nExtended explanations often contrast this with dynamic programming solutions. Fusion-based semantic evaluation ensures that such comparisons are retained."
      },
      {
        "qid": "greedy_algorithm-m-004",
        "short_answer": "Interval partitioning assigns intervals to earliest available resources.",
        "long_answer": "The Interval Partitioning problem focuses on dividing a set of time intervals into the minimum number of groups such that no two intervals in the same group overlap. Unlike interval scheduling, the goal here is not to maximize intervals in one group but to minimize the total number of groups required.\n\nA greedy solution sorts all intervals by their start times. As each interval is processed, it is assigned to a group whose current interval finishes earliest, provided there is no overlap. This is efficiently implemented using a min-heap that tracks end times of all active groups.\n\nIf no existing group can accommodate the interval, a new group is created. The maximum number of groups used at any point corresponds to the answer.\n\nThis greedy strategy is optimal because it always reuses resources whenever possible, minimizing future conflicts.\n\nExtended discussions often relate this problem to meeting room allocation and prove correctness using exchange arguments. Fusion-based semantic evaluation ensures that these later-stage insights are not lost."
      },
      {
        "qid": "greedy_algorithm-m-005",
        "short_answer": "Stock profit is maximized by summing all positive price differences.",
        "long_answer": "The Stock Buy and Sell problem with multiple transactions allows buying and selling a stock any number of times, provided that you sell before buying again. The objective is to maximize total profit.\n\nA greedy approach observes that any increasing price sequence can be decomposed into multiple profitable transactions. Therefore, the optimal strategy is to accumulate profit whenever the price increases from one day to the next.\n\nThis approach effectively captures all upward trends without explicitly identifying local minima and maxima. It runs in O(n) time and uses O(1) extra space.\n\nAlthough this solution may appear simplistic, it is mathematically equivalent to selecting all profitable transactions.\n\nExtended answers often include proof sketches and comparisons with dynamic programming solutions for restricted transaction cases. Fusion-based semantic evaluation ensures that such reasoning is preserved."
      },
      {
        "qid": "greedy_algorithm-m-006",
        "short_answer": "Prim’s algorithm greedily expands the MST from a starting vertex.",
        "long_answer": "Prim’s algorithm constructs a Minimum Spanning Tree by starting from an arbitrary vertex and repeatedly adding the minimum weight edge that connects the growing tree to a new vertex. The greedy choice is always selecting the cheapest edge that expands the current tree.\n\nA priority queue is used to efficiently select the minimum-weight edge among all edges crossing the cut between the tree and the remaining vertices. Each time a vertex is added, its outgoing edges are considered for future selection.\n\nThe time complexity is O(E log V) when implemented with a binary heap. Prim’s algorithm performs particularly well on dense graphs where many edges exist.\n\nCorrectness follows from the cut property, which guarantees that the minimum-weight edge crossing any cut is safe to include in the MST.\n\nExtended explanations often compare Prim’s with Kruskal’s algorithm and discuss different heap optimizations. Fusion-based semantic evaluation ensures deeper algorithmic reasoning is retained."
      },
      {
        "qid": "greedy_algorithm-m-007",
        "short_answer": "Two-pointer greedy maximizes container water area.",
        "long_answer": "The Container With Most Water problem requires finding two vertical lines such that, together with the x-axis, they form a container holding the maximum amount of water. The brute-force solution checks all pairs and runs in O(n²) time, which is inefficient.\n\nA greedy two-pointer approach significantly improves performance. Two pointers are initialized at the beginning and end of the array. At each step, the area is computed using the shorter of the two heights, since the shorter line limits the container’s height.\n\nThe greedy choice is to move the pointer corresponding to the shorter line inward. Moving the taller line cannot increase area because height would still be limited by the shorter one. This ensures no optimal solution is missed.\n\nThe algorithm runs in O(n) time and O(1) space.\n\nExtended explanations often include geometric intuition and correctness proofs. Fusion-based semantic evaluation ensures that such reasoning is preserved even when answers exceed token limits."
      },
      {
        "qid": "greedy_algorithm-m-008",
        "short_answer": "Minimum stick connection cost is achieved via greedy merging.",
        "long_answer": "The Minimum Cost to Connect Sticks problem involves repeatedly combining sticks until only one stick remains, where each combination incurs a cost equal to the sum of the two sticks being merged.\n\nA greedy strategy uses a min-heap to always combine the two smallest sticks first. This choice minimizes the immediate cost and also reduces the impact of future merges, since merged sticks participate in subsequent combinations.\n\nThis approach is mathematically identical to the construction of a Huffman tree and guarantees minimum total cost.\n\nThe algorithm runs in O(n log n) time due to heap operations and uses O(n) space.\n\nExtended discussions often draw parallels with optimal merge patterns and data compression. Fusion-based semantic evaluation ensures these algorithmic insights are not lost."
      },
      {
        "qid": "greedy_algorithm-m-009",
        "short_answer": "Non-overlapping intervals are selected by earliest finish time.",
        "long_answer": "The Non-overlapping Intervals problem asks for the minimum number of intervals to remove so that the remaining intervals do not overlap. Instead of directly selecting removals, a greedy solution focuses on maximizing the number of non-overlapping intervals.\n\nIntervals are sorted by their end times. The greedy choice is to keep the interval with the earliest ending time whenever an overlap occurs. This choice leaves maximum room for future intervals.\n\nEach time an interval overlaps with the previously selected one, the interval with the later end time is removed. This strategy guarantees the minimum number of removals.\n\nThe algorithm runs in O(n log n) time due to sorting and requires constant extra space.\n\nExtended explanations often relate this to classic interval scheduling and include correctness proofs. Fusion-based semantic evaluation ensures that such reasoning is retained."
      },
      {
        "qid": "greedy_algorithm-m-010",
        "short_answer": "Cookies are assigned greedily to satisfy children.",
        "long_answer": "The Assign Cookies problem involves assigning cookies of different sizes to children with varying greed factors. Each child is satisfied if they receive a cookie whose size is greater than or equal to their greed factor.\n\nA greedy solution sorts both the greed factors and cookie sizes in ascending order. Using two pointers, the algorithm attempts to satisfy the least greedy child with the smallest available cookie that meets the requirement.\n\nIf the current cookie satisfies the child, both pointers advance. Otherwise, the cookie pointer advances to try a larger cookie. This ensures that larger cookies are reserved for greedier children.\n\nThe algorithm runs in O(n log n) time due to sorting and uses O(1) extra space.\n\nExtended answers often include proof arguments showing that assigning smaller cookies first never reduces the number of satisfied children. Fusion-based semantic evaluation preserves these justifications."
      },
      {
        "qid": "greedy_algorithm-h-001",
        "short_answer": "Weighted job scheduling combines greedy ordering with optimization.",
        "long_answer": "The Weighted Job Scheduling problem extends the classic job scheduling problem by associating each job with a profit and requiring that no overlapping jobs be selected. The objective is to maximize total profit rather than the number of jobs.\n\nA greedy intuition suggests sorting jobs by finishing time or profit, but pure greedy selection fails because choosing a locally profitable job may block multiple moderately profitable jobs later. Therefore, this problem highlights the limitation of greedy algorithms when optimal substructure is not directly exploitable by local decisions.\n\nPractical solutions combine greedy preprocessing with dynamic programming. Jobs are sorted by finish time, and for each job, the latest non-overlapping job is found using binary search. The algorithm then decides whether to include or exclude the current job based on maximum achievable profit.\n\nThis problem is often used to illustrate why greedy alone is insufficient and how hybrid approaches emerge in algorithm design.\n\nExtended explanations discuss correctness proofs, binary search optimizations, and real-world scheduling applications. Fusion-based semantic evaluation ensures that such higher-level algorithmic reasoning is preserved."
      },
      {
        "qid": "greedy_algorithm-h-002",
        "short_answer": "Minimum window substring uses greedy shrinking of valid windows.",
        "long_answer": "The Minimum Window Substring problem asks for the smallest substring of a given string that contains all characters of another string. Although often categorized under sliding window techniques, it relies on greedy decisions to shrink the window optimally.\n\nThe algorithm expands the right pointer until all required characters are included, then greedily contracts the left pointer while maintaining validity. This greedy shrinking ensures that the window is minimal for each right boundary.\n\nThe correctness comes from the fact that removing unnecessary characters as early as possible never eliminates a valid smaller window. Frequency maps are used to track character counts efficiently.\n\nThe algorithm runs in O(n) time where n is the length of the string, making it highly efficient.\n\nExtended answers often discuss edge cases, optimizations, and comparisons with brute-force methods. Fusion-based semantic evaluation ensures that nuanced window-maintenance logic is not lost."
      },
      {
        "qid": "greedy_algorithm-h-003",
        "short_answer": "Greedy heuristics approximate TSP efficiently.",
        "long_answer": "The Traveling Salesman Problem (TSP) is NP-hard, making exact solutions impractical for large instances. Greedy approximation algorithms provide efficient, though suboptimal, solutions.\n\nOne common greedy heuristic is the nearest neighbor algorithm, which repeatedly visits the closest unvisited city. While simple and fast, it may produce poor solutions in worst cases.\n\nMore advanced greedy-based approximations include MST-based heuristics, where a Minimum Spanning Tree is constructed and traversed to form a tour. These approaches offer provable approximation guarantees under certain conditions.\n\nAlthough greedy approximations do not always yield optimal tours, they are widely used due to their scalability.\n\nExtended discussions often include approximation ratios, counterexamples, and practical performance observations. Fusion-based semantic evaluation ensures that such detailed analysis is preserved."
      },
      {
        "qid": "greedy_algorithm-h-004",
        "short_answer": "Deadline scheduling uses greedy ordering by penalties or deadlines.",
        "long_answer": "The Scheduling with Deadlines and Penalties problem involves selecting and ordering jobs so that the total penalty incurred by missing deadlines is minimized. Each job has a processing time, deadline, and penalty for lateness.\n\nA greedy approach sorts jobs based on penalty-to-processing-time ratio or, in the special case of unit-time jobs, by decreasing penalty. The idea is to prioritize jobs whose late completion would incur the highest cost.\n\nFor unit-time jobs, a common greedy strategy assigns jobs to the latest available time slot before their deadline. If no slot is available, the job is discarded or scheduled late, incurring penalty.\n\nThis problem illustrates how greedy strategies can be adapted based on problem constraints and highlights the importance of choosing the correct greedy criterion.\n\nExtended explanations often discuss proof techniques, alternative formulations, and relationships to scheduling theory. Fusion-based semantic evaluation ensures these deeper insights are not truncated."
      },
      {
        "qid": "greedy_algorithm-h-005",
        "short_answer": "Optimal merge pattern minimizes total merge cost greedily.",
        "long_answer": "The Optimal Merge Pattern problem aims to minimize the total cost of merging multiple files into a single file, where the cost of merging two files is proportional to their sizes.\n\nA greedy solution repeatedly merges the two smallest files first. This choice minimizes the immediate merge cost and also reduces the size of intermediate files, which affects future merge costs.\n\nThe algorithm is implemented using a min-heap to efficiently extract the two smallest files at each step. This greedy strategy is mathematically equivalent to constructing a Huffman tree.\n\nThe algorithm runs in O(n log n) time and guarantees the minimum total merge cost.\n\nExtended answers often relate this problem to data compression and optimal prefix codes. Fusion-based semantic evaluation ensures such connections are preserved."
      },
      {
        "qid": "greedy_algorithm-h-006",
        "short_answer": "Greedy arrow placement minimizes burst operations.",
        "long_answer": "The Minimum Number of Arrows to Burst Balloons problem involves intervals representing balloon ranges on a number line. An arrow shot at a point bursts all balloons whose intervals include that point.\n\nA greedy strategy sorts balloons by their ending coordinates and always shoots an arrow at the end of the earliest-ending balloon. This arrow bursts as many balloons as possible.\n\nOnce an arrow is placed, all balloons it bursts are removed, and the process repeats with the remaining balloons. This greedy choice ensures the minimum number of arrows is used.\n\nThe algorithm runs in O(n log n) time due to sorting and is widely used as an example of interval-based greedy optimization.\n\nExtended discussions often include correctness proofs using exchange arguments. Fusion-based semantic evaluation ensures that these logical justifications are retained."
      },
      {
        "qid": "greedy_algorithm-h-007",
        "short_answer": "Video stitching greedily extends coverage using farthest reach.",
        "long_answer": "The Video Stitching problem requires selecting the minimum number of video clips to cover a target time interval [0, T]. Each clip covers a sub-interval, and clips can overlap.\n\nA greedy approach sorts all clips by their start time. Starting from time zero, the algorithm considers all clips that begin at or before the current coverage point and selects the one that extends coverage the farthest. This greedy choice maximizes progress at each step.\n\nOnce the farthest reachable endpoint is chosen, the coverage point is updated, and the process repeats until the target time T is reached or no further progress can be made.\n\nThis strategy is optimal because delaying extension or choosing a clip with smaller reach cannot lead to a better solution.\n\nExtended explanations often include correctness proofs and comparisons with dynamic programming solutions. Fusion-based semantic evaluation ensures that such reasoning is not lost when responses exceed token limits."
      },
      {
        "qid": "greedy_algorithm-h-008",
        "short_answer": "Partition labels greedily finalize segments at last occurrences.",
        "long_answer": "The Partition Labels problem asks for dividing a string into as many parts as possible such that each letter appears in at most one part.\n\nA greedy solution first records the last occurrence of each character in the string. As the string is scanned from left to right, the current partition’s end is extended to the farthest last occurrence of any character seen so far.\n\nWhen the current index reaches this partition end, a partition is finalized. This greedy choice ensures that characters do not appear in multiple partitions while maximizing the number of partitions.\n\nThe algorithm runs in O(n) time and uses O(1) extra space since the alphabet size is fixed.\n\nExtended answers often include correctness proofs based on contradiction. Fusion-based semantic evaluation preserves these logical arguments."
      },
      {
        "qid": "greedy_algorithm-h-009",
        "short_answer": "Itinerary reconstruction uses greedy DFS traversal.",
        "long_answer": "The Reconstruct Itinerary problem involves finding a valid travel route that uses all given airline tickets exactly once and follows the smallest lexicographical order.\n\nA greedy approach combined with depth-first search is used. For each airport, destinations are stored in a min-heap or sorted list so that the lexicographically smallest choice is always selected first.\n\nHierholzer’s algorithm is applied to find an Eulerian path, ensuring that all tickets are used exactly once. The greedy selection of the smallest destination ensures the lexicographically smallest itinerary.\n\nThe algorithm runs in O(E log E) time due to sorting and guarantees correctness even when cycles are present.\n\nExtended explanations often discuss Eulerian path properties and implementation details. Fusion-based semantic evaluation ensures that these advanced insights are preserved."
      },
      {
        "qid": "greedy_algorithm-h-010",
        "short_answer": "Greedy adjustment enforces increasing array with minimal cost.",
        "long_answer": "The Minimum Cost to Make Array Increasing problem requires modifying elements of an array so that it becomes strictly increasing while minimizing total modification cost.\n\nA greedy approach processes the array from left to right, maintaining the minimum required value for each position. If the current element is already greater than or equal to the required value, it is left unchanged. Otherwise, it is increased to the required value, and the cost difference is accumulated.\n\nThis greedy choice ensures that changes are made only when necessary and are kept as small as possible, preventing larger adjustments later.\n\nThe algorithm runs in linear time and is efficient for large arrays.\n\nExtended answers often discuss proof of optimality and comparisons with dynamic programming alternatives. Fusion-based semantic evaluation ensures these deeper discussions are not lost."
      }     
      
]