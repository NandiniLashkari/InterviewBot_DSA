[
    {
        "qid": "Graphs-e-001",
        "short_answer": "A graph consists of vertices connected by edges.",
        "long_answer": "A graph is a fundamental non-linear data structure used to model relationships between entities. It consists of a set of vertices, also called nodes, and a set of edges that connect pairs of vertices. Vertices represent individual entities such as cities, users, or states, while edges represent relationships such as roads, friendships, or transitions.\n\nGraphs are formally represented as G(V, E), where V is the set of vertices and E is the set of edges. Edges may be directed or undirected and may optionally carry weights that represent costs, distances, or capacities. Depending on the application, graphs can model physical networks, abstract relationships, or computational processes.\n\nIn practical systems, graphs appear in many domains including social networks, computer networks, recommendation systems, biological networks, and search engines. For example, in a social network graph, vertices represent users and edges represent friendships or interactions.\n\nGraphs are powerful because they can represent complex, interconnected systems that cannot be modeled effectively using linear data structures like arrays or linked lists. Understanding graphs requires not only knowledge of their components but also an appreciation of how structure affects algorithmic behavior.\n\nIn extended explanations, candidates often discuss different graph types, real-world examples, and why graphs are essential for modeling relationships. If evaluation systems truncate long responses early, they may miss these conceptual foundations. Fusion-based semantic evaluation ensures that the complete reasoning and contextual understanding are preserved."
      },
      {
        "qid": "Graphs-e-002",
        "short_answer": "Directed graphs have edge direction, undirected graphs do not.",
        "long_answer": "The primary distinction between directed and undirected graphs lies in whether edges have an associated direction. In a directed graph, also known as a digraph, each edge has a specific orientation from a source vertex to a destination vertex. This directionality represents one-way relationships, such as following someone on a social media platform or hyperlinks between web pages.\n\nIn contrast, an undirected graph has edges with no direction. An edge simply indicates a mutual or symmetric relationship between two vertices, such as a friendship or a bidirectional road. In undirected graphs, edges are represented as unordered pairs of vertices.\n\nThis difference has significant algorithmic implications. For example, reachability, connectivity, and traversal algorithms behave differently depending on whether edges are directed. Concepts such as in-degree and out-degree exist only in directed graphs, while undirected graphs use a single degree measure.\n\nMany real-world problems naturally map to one representation or the other. Transportation networks, communication links, and dependency graphs often require directed edges, while physical networks like electrical grids or social friendships are commonly modeled as undirected.\n\nExtended explanations may include examples of how algorithms like DFS, BFS, and shortest path methods differ between directed and undirected graphs. Truncating such explanations early can hide these nuances. Fusion-based semantic evaluation captures the full depth of reasoning."
      },
      {
        "qid": "Graphs-e-003",
        "short_answer": "Graphs are stored using adjacency matrices or adjacency lists.",
        "long_answer": "Graphs can be represented in memory using several data structures, with adjacency matrices and adjacency lists being the two most common. The choice of representation has a significant impact on memory usage and algorithm performance.\n\nAn adjacency matrix uses a two-dimensional array where the entry at row i and column j indicates whether an edge exists between vertex i and vertex j. This representation allows constant-time edge existence checks but requires O(V²) space, making it inefficient for large sparse graphs.\n\nAn adjacency list represents the graph as an array of lists, where each vertex stores a list of its neighboring vertices. This approach uses O(V + E) space and is far more efficient for sparse graphs. However, checking whether a specific edge exists may require traversing a list.\n\nThe choice between these representations depends on graph density and required operations. Dense graphs favor adjacency matrices, while sparse graphs benefit from adjacency lists.\n\nExtended discussions often analyze performance trade-offs, memory locality, and real-world constraints. Fusion-based semantic evaluation ensures that these details are not lost due to truncation."
      },
      {
        "qid": "Graphs-e-004",
        "short_answer": "Adjacency matrices use O(V²) space.",
        "long_answer": "An adjacency matrix representation of a graph uses a two-dimensional array of size V × V, where V is the number of vertices. Each cell indicates whether an edge exists between two vertices, and may also store edge weights in weighted graphs.\n\nThe primary advantage of an adjacency matrix is constant-time access. Checking whether an edge exists, adding an edge, or removing an edge can all be done in O(1) time. This makes adjacency matrices attractive for algorithms that require frequent edge existence checks.\n\nHowever, the major drawback is space inefficiency. Even if the graph has very few edges, the matrix still occupies O(V²) memory. This becomes impractical for large graphs with thousands or millions of vertices.\n\nAdditionally, iterating over all neighbors of a vertex requires scanning an entire row, taking O(V) time, which can be inefficient compared to adjacency lists.\n\nExtended explanations often compare adjacency matrices with other representations and discuss when their advantages outweigh their costs. Fusion-based semantic evaluation ensures these trade-offs are fully captured."
      },
      {
        "qid": "Graphs-e-005",
        "short_answer": "Adjacency lists store only existing edges.",
        "long_answer": "An adjacency list representation stores, for each vertex in the graph, a list of vertices that are directly connected to it by edges. This representation is especially efficient for sparse graphs, where the number of edges is much smaller than the maximum possible number of edges.\n\nThe space complexity of an adjacency list is O(V + E), where V is the number of vertices and E is the number of edges. This makes it significantly more memory-efficient than an adjacency matrix for large graphs with relatively few edges.\n\nFrom a performance perspective, adjacency lists allow efficient traversal of neighbors. Algorithms such as BFS and DFS naturally iterate over adjacency lists, making them well-suited for graph traversal tasks. Adding an edge is typically an O(1) operation, while checking for the existence of a specific edge requires scanning the adjacency list of a vertex.\n\nIn real-world systems such as social networks or web graphs, adjacency lists are the preferred representation due to their scalability. Extended explanations often discuss implementation details, memory layouts, and trade-offs between speed and space.\n\nIf evaluation systems truncate responses early, these practical considerations may be lost. Fusion-based semantic evaluation preserves the full explanation."
      },
      {
        "qid": "Graphs-e-006",
        "short_answer": "Weighted graphs assign numerical values to edges.",
        "long_answer": "A weighted graph is a graph in which each edge is associated with a numerical value known as a weight. These weights typically represent costs, distances, times, or capacities, depending on the application domain. Weighted graphs are essential for modeling real-world problems where relationships have varying strengths or costs.\n\nFor example, in a road network, weights may represent distances between cities or travel time. In network routing, weights can represent bandwidth or latency. Algorithms such as Dijkstra’s shortest path algorithm and Prim’s minimum spanning tree algorithm rely on edge weights to compute optimal solutions.\n\nWeights can be positive, zero, or negative. The presence of negative weights introduces additional complexity and requires specialized algorithms like Bellman-Ford. Negative weight cycles can lead to undefined shortest paths.\n\nExtended explanations often explore how weighted graphs differ from unweighted ones in terms of algorithm design and complexity. Fusion-based semantic evaluation ensures these distinctions are fully captured."
      },
      {
        "qid": "Graphs-e-007",
        "short_answer": "Sparse and dense graphs differ in edge count.",
        "long_answer": "Graphs are often classified as sparse or dense based on the number of edges relative to the number of vertices. A sparse graph has relatively few edges compared to the maximum possible number of edges, while a dense graph has many edges, often close to the maximum.\n\nIn a graph with V vertices, the maximum number of edges in an undirected graph is V(V−1)/2. Sparse graphs satisfy E ≪ V², while dense graphs have E close to V². This distinction strongly influences the choice of graph representation and algorithms.\n\nAdjacency lists are ideal for sparse graphs because they store only existing edges, minimizing memory usage. Adjacency matrices are more suitable for dense graphs, where the cost of storing all possible edges is justified by faster edge access.\n\nExtended discussions often analyze algorithmic performance on sparse versus dense graphs and how this affects real-world applications such as network analysis and graph databases. Fusion-based semantic evaluation preserves this reasoning."
      },
      {
        "qid": "Graphs-e-008",
        "short_answer": "A path is a sequence of connected vertices.",
        "long_answer": "In graph theory, a path is defined as a sequence of vertices in which each consecutive pair of vertices is connected by an edge. Paths are fundamental concepts used to describe connectivity and reachability within a graph.\n\nPaths can be classified in several ways. A simple path contains no repeated vertices, while a general path may revisit vertices. The length of a path is typically measured by the number of edges it contains, although in weighted graphs, path length may be defined as the sum of edge weights.\n\nPaths play a central role in many graph algorithms, including shortest path algorithms, connectivity analysis, and cycle detection. Understanding the properties of paths is essential for reasoning about how information or resources flow through a network.\n\nExtended explanations often discuss variations such as shortest paths, longest paths, and simple versus non-simple paths. If evaluation systems truncate answers early, these conceptual distinctions may be lost. Fusion-based semantic evaluation ensures complete understanding is retained."
      },
      {
        "qid": "Graphs-e-009",
        "short_answer": "A cycle starts and ends at the same vertex.",
        "long_answer": "A cycle in a graph is a path that begins and ends at the same vertex while traversing at least one edge. In a simple cycle, no vertices are repeated except for the starting and ending vertex. Cycles are a core structural feature that distinguishes many classes of graphs.\n\nIn undirected graphs, cycles typically require at least three distinct vertices, whereas in directed graphs, cycles can be formed with two vertices or even a single vertex through a self-loop. The presence or absence of cycles has major implications for algorithm design.\n\nFor example, trees are defined as connected graphs with no cycles. Many algorithms, such as topological sorting, require graphs to be acyclic. Detecting cycles is also critical in dependency resolution systems, deadlock detection, and scheduling problems.\n\nExtended explanations often explore different cycle detection techniques using DFS or Union-Find and discuss why cycles complicate certain computations. Fusion-based semantic evaluation ensures that these conceptual details are not lost due to truncation."
      },
      {
        "qid": "Graphs-e-010",
        "short_answer": "Degree counts edges connected to a vertex.",
        "long_answer": "The degree of a vertex in a graph refers to the number of edges incident to that vertex. In undirected graphs, this is simply the count of neighboring vertices connected by edges. In directed graphs, degree is split into in-degree, representing incoming edges, and out-degree, representing outgoing edges.\n\nVertex degree is an important measure used to analyze graph structure and properties. For example, in social networks, a vertex with high degree may represent a highly connected or influential user. In network routing, degree can indicate redundancy or vulnerability.\n\nA fundamental property of undirected graphs is that the sum of degrees of all vertices equals twice the number of edges, since each edge contributes to the degree of two vertices. This is known as the Handshaking Lemma.\n\nExtended explanations often include applications of degree analysis and how degree distributions affect algorithm performance. Fusion-based semantic evaluation preserves these insights."
      },
      {
        "qid": "Graphs-e-011",
        "short_answer": "A connected graph has paths between all vertices.",
        "long_answer": "An undirected graph is said to be connected if there exists a path between every pair of vertices in the graph. Connectivity is a fundamental property that determines whether the graph forms a single cohesive structure or is divided into multiple disconnected components.\n\nIf a graph is not connected, it consists of two or more connected components, each of which is itself a connected subgraph. Many algorithms, such as spanning tree construction and network traversal, assume or require connectivity.\n\nConnectivity analysis is used in numerous applications, including network reliability, clustering, and social network analysis. Determining connected components is often a preliminary step in more complex graph algorithms.\n\nExtended discussions may include algorithms for finding connected components using BFS or DFS and their complexity. Fusion-based semantic evaluation ensures that these foundational concepts are fully retained."
      },
      {
        "qid": "Graphs-e-012",
        "short_answer": "A tree is a connected acyclic graph.",
        "long_answer": "In graph theory, a tree is defined as an undirected graph that is both connected and acyclic. This combination of properties gives trees several important characteristics that distinguish them from general graphs.\n\nA tree with V vertices always contains exactly V−1 edges. There is a unique simple path between any two vertices, and removing any edge disconnects the tree. Conversely, adding any edge creates a cycle.\n\nTrees are widely used in computer science to represent hierarchical structures such as file systems, organizational charts, and syntax trees. Many efficient algorithms rely on the properties of trees to achieve linear-time performance.\n\nExtended explanations often explore spanning trees, rooted trees, and variations such as binary trees. Fusion-based semantic evaluation ensures that these structural insights are preserved."
      },
      {
        "qid": "Graphs-e-013",
        "short_answer": "BFS explores graphs level by level.",
        "long_answer": "Breadth-First Search (BFS) is a fundamental graph traversal algorithm that explores vertices in increasing order of their distance from a chosen source vertex. It uses a queue to ensure that vertices discovered earlier are processed first, resulting in a level-by-level traversal.\n\nBFS is especially important because it finds the shortest path in unweighted graphs. When BFS visits a vertex for the first time, the path used is guaranteed to be the shortest possible in terms of number of edges. This property makes BFS suitable for problems such as minimum hops in networks, maze solving, and broadcasting.\n\nThe algorithm runs in O(V + E) time, where V is the number of vertices and E is the number of edges, because each vertex and edge is processed at most once. Space complexity is O(V) due to the queue and visited structure.\n\nExtended explanations often discuss BFS tree construction, level graphs, and practical applications. Fusion-based semantic evaluation ensures these deeper insights are not lost due to truncation."
      },
      {
        "qid": "Graphs-e-014",
        "short_answer": "DFS explores as deep as possible before backtracking.",
        "long_answer": "Depth-First Search (DFS) is a graph traversal algorithm that explores vertices by going as deep as possible along each branch before backtracking. It can be implemented using recursion or an explicit stack data structure.\n\nDFS is widely used in problems that require exploring all possible paths or analyzing graph structure, such as cycle detection, topological sorting, and finding connected components. Unlike BFS, DFS does not guarantee shortest paths.\n\nThe time complexity of DFS is O(V + E), as every vertex and edge is visited once. The space complexity is O(V) due to the recursion stack or explicit stack.\n\nExtended discussions often include DFS tree classification, discovery and finishing times, and edge types. Fusion-based semantic evaluation ensures these conceptual details are fully preserved."
      },
      {
        "qid": "Graphs-e-015",
        "short_answer": "BFS explores breadth-wise, DFS explores depth-wise.",
        "long_answer": "The key difference between BFS and DFS lies in their exploration strategy. BFS explores the graph breadth-wise, visiting all neighbors of a vertex before moving to the next level. DFS explores depth-wise, going as deep as possible before backtracking.\n\nBecause of this difference, BFS is well suited for shortest-path problems in unweighted graphs, while DFS is better for tasks such as cycle detection, topological sorting, and path enumeration. BFS generally uses more memory due to queue storage, while DFS uses less memory but may reach deep recursion.\n\nChoosing between BFS and DFS depends on the problem requirements, graph size, and constraints. Both algorithms are foundational and serve as building blocks for more advanced graph algorithms.\n\nFusion-based semantic evaluation captures these nuanced differences even when explanations extend beyond typical length limits."
      },
      {
        "qid": "Graphs-e-016",
        "short_answer": "A complete graph connects every vertex pair.",
        "long_answer": "A complete graph is a graph in which every pair of distinct vertices is connected by an edge. In an undirected complete graph with n vertices, the total number of edges is n(n−1)/2, representing maximum possible connectivity.\n\nComplete graphs are useful theoretical constructs for analyzing worst-case scenarios in graph algorithms, as they represent the densest possible graph structure. Many algorithmic complexities are derived assuming complete graphs as upper bounds.\n\nIn practical applications, complete graphs are rare due to scalability issues, but they appear in small problem instances, clique detection, and theoretical proofs.\n\nExtended discussions may explore properties such as chromatic number, cliques, and edge density. Fusion-based semantic evaluation ensures these properties are not truncated."
      },
      {
        "qid": "Graphs-e-017",
        "short_answer": "Bipartite graphs split vertices into two disjoint sets.",
        "long_answer": "A bipartite graph is a graph whose vertices can be divided into two disjoint sets such that no two vertices within the same set are adjacent. Every edge in a bipartite graph connects a vertex from one set to a vertex in the other set.\n\nBipartite graphs are fundamental in modeling relationships between two distinct classes of entities, such as jobs and applicants, students and courses, or tasks and resources. One important property is that bipartite graphs contain no odd-length cycles.\n\nMany important problems, including maximum matching and assignment problems, are efficiently solvable on bipartite graphs using specialized algorithms like Hopcroft–Karp.\n\nExtended explanations often discuss bipartite testing using BFS coloring and real-world applications. Fusion-based semantic evaluation preserves these deeper insights."
      },
      {
        "qid": "Graphs-e-018",
        "short_answer": "Topological sorting orders vertices in DAGs.",
        "long_answer": "Topological sorting produces a linear ordering of vertices in a Directed Acyclic Graph (DAG) such that for every directed edge (u, v), vertex u appears before vertex v in the ordering.\n\nTopological sorting is applicable only to DAGs and is widely used in scheduling, dependency resolution, build systems, and task execution pipelines. Algorithms for topological sorting include Kahn’s algorithm using indegree counting and DFS-based approaches.\n\nIf a graph contains a cycle, topological sorting is not possible, and cycle detection becomes necessary. This makes topological sorting a useful tool for validating dependency graphs.\n\nExtended discussions often include algorithm comparisons and practical use cases. Fusion-based semantic evaluation ensures these conceptual details remain intact."
      },
      {
        "qid": "Graphs-e-019",
        "short_answer": "SCCs group mutually reachable vertices in directed graphs.",
        "long_answer": "A strongly connected component (SCC) in a directed graph is a maximal set of vertices such that every vertex is reachable from every other vertex in the set. SCCs reveal the internal structure of directed graphs and help simplify complex graph problems.\n\nAlgorithms such as Kosaraju’s and Tarjan’s find SCCs in linear time O(V + E). SCC condensation forms a DAG, enabling further analysis like topological sorting.\n\nSCCs are widely used in compiler design, dependency analysis, and network analysis. Understanding SCC structure often simplifies graph reasoning.\n\nFusion-based semantic evaluation captures these structural insights even when explanations exceed token limits."
      },
      {
        "qid": "Graphs-e-020",
        "short_answer": "Trees are acyclic connected graphs, unlike general graphs.",
        "long_answer": "The key difference between a graph and a tree lies in structure and constraints. Trees are connected and acyclic graphs with exactly V−1 edges, while general graphs may contain cycles, multiple components, and varying numbers of edges.\n\nTrees guarantee a unique path between any two vertices, making them ideal for hierarchical data representation. Graphs allow multiple paths and cycles, offering greater modeling flexibility.\n\nMany graph algorithms specialize when applied to trees, achieving improved performance. Understanding this distinction helps in selecting the appropriate data structure for a problem.\n\nFusion-based semantic evaluation ensures that these structural distinctions are preserved in full explanations."
      },
      {
        "qid": "Graphs-m-001",
        "short_answer": "Dijkstra finds shortest paths with non-negative weights.",
        "long_answer": "Dijkstra’s algorithm is a classic greedy algorithm used to compute the shortest path from a single source vertex to all other vertices in a graph with non-negative edge weights. The algorithm maintains a distance array where each entry represents the currently known shortest distance from the source to that vertex.\n\nAt each step, Dijkstra selects the unvisited vertex with the smallest tentative distance and relaxes all outgoing edges from that vertex. Relaxation means updating the distance of neighboring vertices if a shorter path is found through the current vertex. This greedy choice works correctly only when edge weights are non-negative.\n\nThe time complexity of Dijkstra’s algorithm depends on the data structure used. With a simple array, it runs in O(V²). Using a binary heap, the complexity improves to O((V + E) log V). With a Fibonacci heap, it can be further optimized to O(E + V log V), although the constant factors are higher.\n\nIn extended answers, candidates often discuss why negative weights break Dijkstra’s assumptions and compare its performance with other shortest-path algorithms. These deeper explanations typically appear later in long responses.\n\nIf evaluation systems truncate answers early, they may miss these algorithmic nuances. Fusion-based semantic evaluation ensures that the full reasoning behind correctness and complexity is preserved."
      },
      {
        "qid": "Graphs-m-002",
        "short_answer": "Bellman-Ford handles negative weights and cycles.",
        "long_answer": "The Bellman-Ford algorithm is used to find shortest paths from a single source vertex in graphs that may contain negative edge weights. Unlike Dijkstra’s algorithm, Bellman-Ford does not rely on greedy assumptions and instead uses repeated relaxation of all edges.\n\nThe algorithm works by relaxing all edges V−1 times, where V is the number of vertices. This ensures that the shortest path to each vertex, which can contain at most V−1 edges, is found. An additional iteration is used to detect negative weight cycles: if any distance can still be reduced, a negative cycle exists.\n\nThe time complexity of Bellman-Ford is O(VE), making it slower than Dijkstra for large graphs. However, its ability to detect negative cycles makes it indispensable in certain applications such as financial arbitrage detection and constraint satisfaction problems.\n\nExtended explanations often compare Bellman-Ford with Dijkstra and Floyd–Warshall, discussing trade-offs between generality and efficiency. Fusion-based semantic evaluation ensures these comparisons are not lost due to truncation."
      },
      {
        "qid": "Graphs-m-003",
        "short_answer": "Cycle detection differs for directed and undirected graphs.",
        "long_answer": "Cycle detection is a fundamental problem in graph theory, with different techniques depending on whether the graph is directed or undirected. In undirected graphs, depth-first search (DFS) is commonly used along with parent tracking. If a visited vertex is encountered that is not the parent, a cycle exists.\n\nIn directed graphs, cycle detection is more complex due to edge direction. A common approach uses DFS with a recursion stack or a color-based system (white, gray, black). Encountering a gray vertex during DFS indicates a back edge and therefore a cycle.\n\nCycle detection plays a critical role in many applications, including deadlock detection, dependency resolution, and scheduling systems. For example, topological sorting is only possible in directed acyclic graphs, making cycle detection a prerequisite.\n\nExtended discussions often explore algorithm correctness proofs and time complexity analysis. Fusion-based semantic evaluation ensures that these deeper explanations are fully captured."
      },
      {
        "qid": "Graphs-m-004",
        "short_answer": "Kruskal builds MST by adding smallest edges without cycles.",
        "long_answer": "Kruskal’s algorithm is a greedy algorithm used to find a Minimum Spanning Tree (MST) of a connected, weighted, undirected graph. The core idea is to sort all edges by increasing weight and then iteratively add edges to the MST, ensuring that no cycles are formed.\n\nTo efficiently detect cycles, Kruskal’s algorithm relies on the Union-Find (Disjoint Set Union) data structure. Each vertex starts in its own set, and when an edge connects two vertices from different sets, those sets are merged. If the vertices are already in the same set, the edge is skipped to avoid creating a cycle.\n\nThe dominant cost of Kruskal’s algorithm comes from sorting the edges, resulting in a time complexity of O(E log E). With path compression and union by rank, Union-Find operations are nearly constant time. Kruskal’s algorithm is particularly effective for sparse graphs.\n\nExtended explanations often include comparisons with Prim’s algorithm and discussions about edge sorting strategies. Fusion-based semantic evaluation ensures these deeper algorithmic insights are preserved."
      },
      {
        "qid": "Graphs-m-005",
        "short_answer": "Prim’s algorithm grows the MST from a starting vertex.",
        "long_answer": "Prim’s algorithm is another greedy approach for constructing a Minimum Spanning Tree. Unlike Kruskal’s algorithm, which focuses on edges globally, Prim’s algorithm starts from an arbitrary vertex and grows the MST by repeatedly adding the minimum-weight edge that connects the current tree to a new vertex.\n\nPrim’s algorithm maintains a priority queue of candidate edges and selects the smallest edge at each step. Using a binary heap, the time complexity is O(E log V). With adjacency matrices, the complexity becomes O(V²), making it suitable for dense graphs.\n\nThe choice between Prim’s and Kruskal’s algorithms often depends on graph density and representation. Prim’s tends to perform better on dense graphs, while Kruskal’s is more efficient for sparse graphs.\n\nExtended responses often discuss implementation details, such as adjacency list vs matrix representations. Fusion-based semantic evaluation captures these nuanced comparisons."
      },
      {
        "qid": "Graphs-m-006",
        "short_answer": "Union-Find efficiently manages disjoint sets.",
        "long_answer": "The Union-Find data structure, also known as Disjoint Set Union (DSU), is designed to keep track of a partition of elements into disjoint sets. It supports two primary operations: Find, which identifies the set an element belongs to, and Union, which merges two sets.\n\nWith optimizations such as path compression and union by rank, Union-Find operations have nearly constant amortized time complexity. This makes it highly efficient for algorithms that require frequent connectivity checks.\n\nUnion-Find is widely used in graph algorithms, particularly in Kruskal’s MST algorithm, cycle detection in undirected graphs, and connected component analysis. Its efficiency makes it a foundational data structure in algorithm design.\n\nExtended explanations often explore the inverse Ackermann function and its role in complexity analysis. Fusion-based semantic evaluation ensures that these theoretical insights are retained."
      },
      {
        "qid": "Graphs-m-007",
        "short_answer": "Ford–Fulkerson finds maximum flow using augmenting paths.",
        "long_answer": "The Ford–Fulkerson method is an algorithmic framework for computing the maximum flow in a flow network. A flow network consists of vertices connected by directed edges, each with a specified capacity, along with a source and a sink.\n\nThe algorithm works by repeatedly finding an augmenting path from the source to the sink in the residual graph. An augmenting path is a path where every edge has remaining capacity. Once such a path is found, the flow is increased along that path by the minimum residual capacity of its edges. The residual graph is then updated to reflect the new flow values.\n\nThe correctness of Ford–Fulkerson relies on the Max-Flow Min-Cut Theorem, which states that the maximum flow in a network equals the capacity of the minimum cut separating the source and sink. The running time depends on how augmenting paths are chosen. Using arbitrary DFS, the complexity can be O(E × max_flow). When BFS is used (Edmonds–Karp algorithm), the complexity becomes O(VE²).\n\nExtended explanations often discuss residual graphs, flow conservation, and practical limitations of naive augmenting path selection. Fusion-based semantic evaluation ensures that these deeper algorithmic concepts are preserved."
      },
      {
        "qid": "Graphs-m-008",
        "short_answer": "Tarjan’s algorithm finds SCCs in linear time.",
        "long_answer": "Tarjan’s algorithm is a depth-first search based technique for identifying strongly connected components (SCCs) in a directed graph. It efficiently decomposes the graph into SCCs in a single DFS traversal.\n\nThe algorithm assigns each vertex a discovery time and a low-link value, which represents the smallest discovery time reachable from that vertex through DFS tree edges and back edges. A stack is used to keep track of vertices that are part of the current DFS path. When a vertex’s low-link value equals its discovery time, it indicates the root of an SCC, and vertices are popped from the stack to form that component.\n\nTarjan’s algorithm runs in O(V + E) time and uses O(V) space. It is widely used in compiler optimization, program analysis, and network connectivity analysis.\n\nExtended responses often compare Tarjan’s algorithm with Kosaraju’s algorithm and discuss implementation subtleties. Fusion-based semantic evaluation ensures that these details are not lost due to truncation."
      },
      {
        "qid": "Graphs-m-009",
        "short_answer": "Graph coloring assigns colors so adjacent vertices differ.",
        "long_answer": "Graph coloring is the problem of assigning colors to vertices of a graph such that no two adjacent vertices share the same color. The minimum number of colors required to achieve this is known as the chromatic number of the graph.\n\nGraph coloring has many practical applications, including scheduling tasks without conflicts, register allocation in compilers, and frequency assignment in wireless networks. While greedy coloring algorithms provide fast and simple solutions, they do not always yield the minimum number of colors.\n\nDetermining the chromatic number of a general graph is NP-hard. However, special classes of graphs, such as bipartite graphs and interval graphs, have efficient optimal coloring algorithms.\n\nExtended explanations often discuss approximation algorithms, heuristic methods, and theoretical bounds. Fusion-based semantic evaluation ensures that these conceptual insights are fully captured."
      },
      {
        "qid": "Graphs-m-010",
        "short_answer": "TSP seeks the shortest tour visiting each vertex once.",
        "long_answer": "The Traveling Salesman Problem (TSP) is a classic combinatorial optimization problem that asks for the shortest possible route that visits each city exactly once and returns to the starting city. TSP is fundamental in graph theory and theoretical computer science because it is NP-hard.\n\nExact solutions to TSP include brute-force enumeration with time complexity O(n!), which is infeasible for large n, and dynamic programming solutions such as the Held–Karp algorithm with complexity O(n²·2^n). These methods guarantee optimal solutions but are limited to small input sizes.\n\nDue to its computational difficulty, many approximation and heuristic approaches are used in practice. Greedy heuristics like nearest neighbor provide fast but suboptimal solutions, while more advanced algorithms such as Christofides’ algorithm guarantee a 1.5-approximation for metric TSP.\n\nExtended explanations often include discussions of real-world applications, approximation guarantees, and why TSP remains computationally hard despite decades of research. Fusion-based semantic evaluation ensures that these deeper algorithmic insights are not lost due to truncation."
      },
      {
        "qid": "Graphs-h-001",
        "short_answer": "Graph isomorphism checks structural equivalence of graphs.",
        "long_answer": "The graph isomorphism problem asks whether two graphs are structurally identical, meaning there exists a bijection between their vertex sets that preserves adjacency. Unlike many classical problems, graph isomorphism occupies a unique position in computational complexity theory.\n\nIt is known to be in NP, but it is neither proven to be NP-complete nor shown to be solvable in polynomial time. Recent breakthroughs have placed graph isomorphism in quasi-polynomial time, significantly narrowing the gap between theoretical and practical solvability.\n\nThe importance of graph isomorphism lies in applications such as chemical compound comparison, pattern recognition, circuit design verification, and database indexing. Efficiently determining isomorphism can reduce redundant computations and identify equivalent structures.\n\nExtended answers often discuss canonical labeling, Weisfeiler–Lehman refinement, and the relationship between graph isomorphism and the P vs NP question. These deeper theoretical discussions usually appear later in long explanations.\n\nIf evaluation systems truncate answers early, they may miss the broader complexity-theoretic implications. Fusion-based semantic evaluation ensures that the full conceptual depth is preserved."
      },
      {
        "qid": "Graphs-h-002",
        "short_answer": "Push–relabel improves max-flow performance on dense graphs.",
        "long_answer": "The push–relabel algorithm is an advanced technique for computing maximum flow in a network. Unlike augmenting-path algorithms such as Ford–Fulkerson, push–relabel maintains a preflow that allows temporary violations of flow conservation at intermediate vertices.\n\nEach vertex is assigned a height, and flow is pushed locally along admissible edges to neighboring vertices with lower height. When no admissible edges exist, the vertex is relabeled by increasing its height. This local operation strategy enables efficient handling of dense graphs.\n\nTheoretical analysis shows that push–relabel runs in O(V²E) time, with practical optimizations such as gap relabeling and global relabeling significantly improving real-world performance. In practice, push–relabel often outperforms classical max-flow algorithms.\n\nExtended explanations often include proofs of correctness, discussion of preflows, and comparisons with Edmonds–Karp. Fusion-based semantic evaluation ensures these algorithmic nuances are retained."
      },
      {
        "qid": "Graphs-h-003",
        "short_answer": "Matching algorithms pair vertices without conflicts.",
        "long_answer": "Matching problems in graphs involve selecting a set of edges such that no two edges share a common vertex. In bipartite graphs, this problem can be solved efficiently using algorithms like Hopcroft–Karp, which runs in O(E√V) time.\n\nFor general graphs, the presence of odd-length cycles complicates matching. Edmonds’ Blossom algorithm addresses this by contracting odd cycles into single super-nodes, enabling polynomial-time solutions for maximum matching in general graphs.\n\nMatching algorithms are foundational in applications such as job assignment, scheduling, network routing, and stable marriage problems. Weighted matching variants further extend applicability by maximizing total edge weight.\n\nExtended discussions often explore correctness proofs, implementation challenges, and complexity trade-offs. Fusion-based semantic evaluation ensures that these deeper algorithmic insights are fully captured."
      },
      {
        "qid": "Graphs-h-004",
        "short_answer": "Johnson’s algorithm enables APSP with negative edges.",
        "long_answer": "Johnson’s algorithm computes all-pairs shortest paths (APSP) in sparse graphs that may contain negative edge weights but no negative cycles. The algorithm combines the strengths of Bellman–Ford and Dijkstra’s algorithms to achieve better performance than Floyd–Warshall on large sparse graphs.\n\nThe process begins by adding a super-source connected to all vertices with zero-weight edges and running Bellman–Ford to compute vertex potentials. These potentials are then used to reweight all edges so that every edge weight becomes non-negative while preserving shortest-path relationships.\n\nAfter reweighting, Dijkstra’s algorithm is run from each vertex to compute shortest paths efficiently. The overall time complexity is O(VE + V^2 log V) when using a binary heap, which is significantly better than O(V^3) for Floyd–Warshall on sparse graphs.\n\nExtended explanations often discuss why reweighting preserves shortest paths and compare practical trade-offs between APSP algorithms. Fusion-based semantic evaluation ensures that these algorithmic insights are fully preserved."
      },
      {
        "qid": "Graphs-h-005",
        "short_answer": "Planar graphs can be drawn without edge crossings.",
        "long_answer": "A planar graph is one that can be drawn on a plane such that no two edges intersect except at their endpoints. Planar graphs have rich structural properties that enable efficient algorithms for many otherwise hard problems.\n\nA fundamental result is Euler’s formula, which states that for a connected planar graph, V − E + F = 2, where V is the number of vertices, E the number of edges, and F the number of faces. From this, it follows that planar graphs have at most 3V − 6 edges.\n\nKuratowski’s theorem characterizes planar graphs as those that do not contain K5 or K3,3 as minors. Efficient linear-time algorithms exist for planarity testing and embedding.\n\nExtended discussions often include applications in circuit layout, geographic mapping, and graph drawing. Fusion-based semantic evaluation ensures these theoretical and practical insights are not lost."
      },
      {
        "qid": "Graphs-h-006",
        "short_answer": "Approximation algorithms tackle NP-hard graph problems.",
        "long_answer": "Many important graph problems are NP-hard, making exact solutions infeasible for large inputs. Approximation algorithms aim to find near-optimal solutions within provable bounds on solution quality.\n\nExamples include the 2-approximation algorithm for vertex cover, Christofides’ 1.5-approximation for metric Traveling Salesman Problem, and logarithmic-factor approximations for set cover. These algorithms provide guarantees that the solution will not exceed a certain factor of the optimal.\n\nThe study of approximation algorithms involves analyzing trade-offs between runtime, approximation ratio, and problem structure. Some problems are provably hard to approximate within any constant factor unless P = NP.\n\nExtended explanations often discuss inapproximability results and practical heuristic alternatives. Fusion-based semantic evaluation ensures these nuanced theoretical insights are preserved."
      },
      {
        "qid": "Graphs-h-007",
        "short_answer": "Graph minor theory studies containment via contractions.",
        "long_answer": "Graph minor theory investigates whether one graph can be obtained from another by a sequence of vertex deletions, edge deletions, and edge contractions. This notion of containment leads to deep structural insights into graph families.\n\nThe Robertson–Seymour theorem is a landmark result in this area, stating that any minor-closed family of graphs can be characterized by a finite set of forbidden minors. This theorem has far-reaching consequences in algorithmic graph theory.\n\nOne of the most important implications is that many graph problems that are NP-hard in general become polynomial-time solvable when restricted to graphs excluding a fixed minor. Examples include problems on planar graphs and graphs of bounded treewidth.\n\nExtended discussions often explore algorithmic meta-theorems, treewidth, and applications in parameterized complexity. Fusion-based semantic evaluation ensures these deep theoretical insights are not lost due to truncation."
      },
      {
        "qid": "Graphs-h-008",
        "short_answer": "Dynamic graphs support updates while maintaining properties.",
        "long_answer": "Dynamic graph algorithms are designed to handle graphs that change over time through edge insertions and deletions. Unlike static algorithms, dynamic algorithms must efficiently update solutions without recomputing everything from scratch.\n\nProblems studied in dynamic graph settings include connectivity, shortest paths, minimum spanning trees, and reachability. Depending on the type of updates allowed, algorithms are classified as incremental (insertions only), decremental (deletions only), or fully dynamic.\n\nAdvanced data structures such as link-cut trees, Euler tour trees, and dynamic trees enable logarithmic-time updates for certain problems. These structures trade increased implementation complexity for improved performance.\n\nExtended explanations often discuss update-query trade-offs, amortized complexity, and real-world applications such as network monitoring. Fusion-based semantic evaluation preserves these nuanced discussions."
      },
      {
        "qid": "Graphs-h-009",
        "short_answer": "Parallel graph algorithms face scalability challenges.",
        "long_answer": "Parallel algorithms for graph problems aim to leverage multiple processors to speed up computation. However, graphs pose unique challenges due to irregular structure, unpredictable memory access patterns, and data dependencies between vertices.\n\nProblems such as BFS, shortest paths, and connectivity have parallel implementations, but achieving good scalability depends heavily on graph structure. Load imbalance, synchronization overhead, and communication costs often limit speedup.\n\nSome graph problems, like depth-first search, are inherently sequential and difficult to parallelize efficiently. Others allow partial parallelism through techniques such as frontier-based exploration or graph partitioning.\n\nExtended discussions often analyze theoretical speedup limits and practical performance on modern hardware. Fusion-based semantic evaluation ensures these deeper insights remain intact."
      },
      {
        "qid": "Graphs-h-010",
        "short_answer": "Spectral graph theory studies graphs via eigenvalues.",
        "long_answer": "Spectral graph theory analyzes graphs using the eigenvalues and eigenvectors of matrices associated with graphs, such as the adjacency matrix and the Laplacian matrix. These spectral properties reveal deep structural information that is not always apparent from combinatorial representations alone.\n\nOne of the central concepts is the graph Laplacian, whose eigenvalues encode information about connectivity, expansion, and clustering. For example, the second smallest eigenvalue of the Laplacian, known as the algebraic connectivity, measures how well connected the graph is. Larger values indicate stronger connectivity and robustness to vertex or edge removal.\n\nSpectral methods are widely used in applications such as graph partitioning, community detection, image segmentation, and recommendation systems. Algorithms based on spectral clustering leverage eigenvectors to embed graph vertices into lower-dimensional spaces where clusters become separable.\n\nTheoretical results like Cheeger’s inequality connect spectral gaps to combinatorial notions such as conductance, providing guarantees on the quality of partitions produced by spectral algorithms. These connections bridge linear algebra, probability, and combinatorics.\n\nIf evaluation systems truncate long responses early, they may miss the discussion of eigenvalue interpretations, algorithmic applications, and theoretical guarantees. Fusion-based semantic evaluation ensures that the full depth of spectral reasoning is preserved, making it especially suitable for assessing long, technically rich answers."
      }
      
      
      
      
      
      
      
      
      
      
      
      
      
]